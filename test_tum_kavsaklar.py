import os
import sys
import gymnasium as gym
import numpy as np

# Windows Libsumo ayarÄ± (HÄ±z iÃ§in)
if os.name != 'nt':
    os.environ['LIBSUMO_AS_TRACI'] = '1'

from stable_baselines3 import PPO
# --- KRÄ°TÄ°K Ä°MPORTLAR ---
from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack
import sumo_rl

# --- AYARLAR ---
# 2 Åžeritli harita ile eÄŸittiysen burayÄ± ona gÃ¶re gÃ¼ncelle!
HARITA_DOSYASI = "SUMO\map\grid_sehir.net.xml"
TRAFIK_DOSYASI = "SUMO\map\\traffic.rou.xml"

# EÄŸittiÄŸin son modelin tam adÄ± (uzantÄ±sÄ±z)
MODEL_DOSYASI = "trafik_yonetici_hafizali" 

# EÄŸitimdeki ayar neyse o olmalÄ± (10 veya 15)
KARAR_SURESI = 10 

# --- EÄžÄ°TÄ°MDE KULLANDIÄžIMIZ ADAPTER SINIFI ---
# Bu sÄ±nÄ±f olmadan model Ã§alÄ±ÅŸmaz, Ã§Ã¼nkÃ¼ model bu sÄ±nÄ±ftan gelen veriye gÃ¶re eÄŸitildi.
class PettingZooToGymAdapter(gym.Env):
    def __init__(self, pz_env):
        self.pz_env = pz_env
        self.possible_agents = pz_env.possible_agents
        self.observation_space = pz_env.observation_space(self.possible_agents[0])
        self.action_space = pz_env.action_space(self.possible_agents[0])
        self.render_mode = "rgb_array"
        self.metadata = {"render_modes": ["rgb_array"]}
        self.last_action = None 

    def reset(self, seed=None, options=None):
        self.last_action = None
        obs_dict, info_dict = self.pz_env.reset(seed=seed, options=options)
        return obs_dict[self.possible_agents[0]], info_dict[self.possible_agents[0]]

    def step(self, action):
        # Tek aksiyonu tÃ¼m ajanlara yay (Parameter Sharing)
        actions = {agent: action for agent in self.possible_agents}
        obs_dict, rewards, terminations, truncations, infos = self.pz_env.step(actions)
        
        # Sadece izliyoruz, Ã¶dÃ¼l hesaplamaya gerek yok ama format bozulmasÄ±n
        total_reward = sum(rewards.values())
        self.last_action = action
        
        obs = obs_dict[self.possible_agents[0]]
        done = any(terminations.values()) or any(truncations.values())
        info = infos[self.possible_agents[0]]
        return obs, total_reward, done, False, info

def main():
    print("ðŸ§  HAFIZALI MODEL TEST EDÄ°LÄ°YOR...")
    print(f"Harita: {HARITA_DOSYASI}")

    # 1. TEMEL ORTAMI OLUÅžTUR (GUI AÃ‡IK)
    # Burada direkt sumo_rl.SumoEnvironment deÄŸil, parallel_env kullanÄ±yoruz
    # Ã§Ã¼nkÃ¼ Adapter sÄ±nÄ±fÄ±mÄ±z parallel_env bekliyor.
    env = sumo_rl.parallel_env(
        net_file=HARITA_DOSYASI,
        route_file=TRAFIK_DOSYASI,
        use_gui=True,              # <--- Ä°ZLEMEK Ä°Ã‡Ä°N AÃ‡IK
        num_seconds=3600,
        min_green=5,
        delta_time=KARAR_SURESI,
        reward_fn='pressure',
    )

    # 2. ADAPTER Ä°LE SAR
    env = PettingZooToGymAdapter(env)

    # 3. VEKTÃ–R ORTAMI YAP (SB3 Uyumu iÃ§in)
    # VecFrameStack kullanabilmek iÃ§in ortamÄ±n DummyVecEnv olmasÄ± ÅŸarttÄ±r.
    env = DummyVecEnv([lambda: env])

    # 4. HAFIZA EKLE (VecFrameStack)
    # --- EN Ã–NEMLÄ° KISIM BURASI ---
    # Model 4 kare hafÄ±zalÄ± eÄŸitildiÄŸi iÃ§in testte de 4 kare vermeliyiz.
    env = VecFrameStack(env, n_stack=4)

    # 5. MODELÄ° YÃœKLE
    try:
        model = PPO.load(MODEL_DOSYASI)
        print("âœ… HafÄ±zalÄ± Beyin YÃ¼klendi.")
    except FileNotFoundError:
        print(f"âŒ HATA: '{MODEL_DOSYASI}.zip' bulunamadÄ±.")
        return

    # 6. SÄ°MÃœLASYONU BAÅžLAT
    obs = env.reset() # VecEnv olduÄŸu iÃ§in direkt obs dÃ¶ner (info dÃ¶nmez)
    
    done = False
    step = 0
    
    while not done:
        # Modelden karar iste
        action, _states = model.predict(obs, deterministic=True)
        
        # Konsola yaz
        print(f"AdÄ±m {step} -> Karar: {action[0]}") # VecEnv olduÄŸu iÃ§in action bir liste gelir
        
        # KararÄ± uygula
        obs, rewards, done, info = env.step(action)
        
        step += 1

    print("Test bitti.")
    env.close()

if __name__ == "__main__":
    main()