import os
import sys
import gymnasium as gym

# Windows HÄ±zlandÄ±rmasÄ±
if os.name != 'nt':
    os.environ['LIBSUMO_AS_TRACI'] = '1'

from stable_baselines3 import PPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import VecMonitor
import sumo_rl
import supersuit as ss

# --- AYARLAR ---
# DÃœZELTME: Dosya yollarÄ±nÄ±n baÅŸÄ±na 'r' koyduk (Raw String).
# BÃ¶ylece \ iÅŸaretleri sorun Ã§Ä±karmaz.
HARITA_DOSYASI = r"SUMO\mapV2\duz_yol.net.xml"
TRAFIK_DOSYASI = r"SUMO\mapV2\duz_map.rou.xml"
MODEL_ADI = "trafik_yonetici_bagimsiz"

# MANTIK AYARLARI
KARAR_SURESI = 10 
MIN_YESIL = 5
SIMULASYON_SURESI = 4000
ISLEM_SAYISI = 4   # Ã‡ekirdek SayÄ±sÄ±

def main():
    print(f"ğŸš€ BAÄIMSIZ MULTI-AGENT EÄÄ°TÄ°M ({ISLEM_SAYISI} Ã‡ekirdek)...")
    print("Not: Bu sefer her kavÅŸak KENDÄ° kararÄ±nÄ± verecek.")

    # 1. ORTAMI OLUÅTUR (SÄ±nÄ±f miras alma YOK)
    # Direkt fonksiyonu Ã§aÄŸÄ±rÄ±yoruz.
    env = sumo_rl.parallel_env(
        net_file=HARITA_DOSYASI,
        route_file=TRAFIK_DOSYASI,
        use_gui=False,
        num_seconds=SIMULASYON_SURESI,
        min_green=MIN_YESIL,
        delta_time=KARAR_SURESI,
        reward_fn='pressure', 
    )

    # --- HATA DÃœZELTME YAMASI (INSTANCE PATCHING) ---
    # SÄ±nÄ±f oluÅŸturmak yerine, oluÅŸturulmuÅŸ nesneye (env)
    # eksik olan Ã¶zelliÄŸi elle yapÄ±ÅŸtÄ±rÄ±yoruz.
    try:
        env.unwrapped.render_mode = "rgb_array"
    except AttributeError:
        env.render_mode = "rgb_array"
    # ------------------------------------------------

    # 2. BAÄIMSIZLAÅTIRMA VE HIZLANDIRMA (SuperSuit)
    
    # AdÄ±m A: PettingZoo -> VektÃ¶r OrtamÄ±
    # Bu aÅŸamada ortam PPO uyumlu hale gelir.
    env = ss.pettingzoo_env_to_vec_env_v1(env)
    
    # AdÄ±m B: Ä°ÅŸlemcilere DaÄŸÄ±t (ParalelleÅŸtirme)
    # concat_vec_envs_v1 fonksiyonu bizim iÃ§in 4 tane iÅŸlemci aÃ§ar.
    # num_vec_envs=ISLEM_SAYISI: Toplam kaÃ§ simÃ¼lasyon dÃ¶necek?
    # num_cpus=ISLEM_SAYISI: KaÃ§ Ã§ekirdek kullanacak?
    env = ss.concat_vec_envs_v1(env, num_vec_envs=ISLEM_SAYISI, num_cpus=ISLEM_SAYISI, base_class='stable_baselines3')

    # 3. LOGLAMA
    env = VecMonitor(env)

    # 4. MODEL (MeraklÄ± PPO)
    model = PPO(
        "MlpPolicy", 
        env, 
        verbose=1,
        learning_rate=0.0003,
        batch_size=1024,      
        n_steps=512,
        
        # --- KRÄ°TÄ°K AYAR: ENTROPÄ° ---
        # 0.05 yaparak modelin "farklÄ± ÅŸeyler denemesini" saÄŸlÄ±yoruz.
        # Bu sayede Ä±ÅŸÄ±klar senkronize (aynÄ± anda) hareket etmez.
        ent_coef=0.05,        
        
        gamma=0.995,
        device='auto'
    )

    # 5. EÄÄ°TÄ°M
    EGITIM_ADIM = 1000000 
    print(f"Hedef: {EGITIM_ADIM} adÄ±m. BaÅŸlÄ±yor...")
    
    model.learn(total_timesteps=EGITIM_ADIM)

    model.save(MODEL_ADI)
    print(f"\nâœ… BaÄŸÄ±msÄ±z Model EÄŸitildi! '{MODEL_ADI}.zip'")
    env.close()

if __name__ == "__main__":
    main()